太棒了—我把整套**工业级**“Transformer 消融实验（WMT14）”工程按“文件分类（file category）+ 分文件（file by file）”重构好了。
特点如下（全部代码不省略）：

* PyTorch + tensorboardX + loguru + sentencepiece + sacrebleu + datasets（可回退本地 TSV）
* 严格类型标注、Google 风格 Docstring（PEP 257）、PEP 8 命名与格式
* 关键步骤**中文注释**（按 PEP 8 风格放在代码行上方或行尾）
* 可做**消融**：注意力头数、模型维度、Dropout、位置编码（正弦 vs 可学习）
* 训练技巧：Adam + Noam Warmup、Label Smoothing、权重共享、Mixed Precision、梯度裁剪
* 动态 Padding + 掩码严谨处理、贪心解码 + SacreBLEU 评估
* 健壮的**边界检查** + **Sanity Check** 脚本
* 统一日志与可视化，自动保存最佳权重并汇总结果

> 目录结构（建议拷贝为工程根目录）：

```
transformer_ablation/
  ├─ main.py
  ├─ config.py
  ├─ requirements.txt
  ├─ data/
  │   ├─ __init__.py
  │   └─ datamodule.py
  ├─ models/
  │   ├─ __init__.py
  │   ├─ layers.py
  │   ├─ transformer.py
  │   ├─ losses.py
  │   └─ schedulers.py
  ├─ training/
  │   ├─ __init__.py
  │   ├─ trainer.py
  │   └─ ablation.py
  ├─ utils/
  │   ├─ __init__.py
  │   ├─ logging.py
  │   ├─ seed.py
  │   └─ io.py
  └─ scripts/
      └─ run_sanity.py
```

---

# File category: Project Root

## `requirements.txt`

```text
torch>=2.1
tensorboardX>=2.6
loguru>=0.7
datasets>=2.19
sentencepiece>=0.2
sacrebleu>=2.4
numpy>=1.24
```

## `config.py`

```python
# -*- coding: utf-8 -*-
"""Experiment configuration dataclass.

This module defines ExperimentConfig used across the project.

Author: GPT-5 Thinking
"""

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Literal, Optional


@dataclass
class ExperimentConfig:
    """Experiment configuration.

    Attributes:
        run_name: Unique run name (for logging / checkpoints).
        log_dir: TensorBoard & log directory.
        save_dir: Checkpoint directory.
        data_dir: Local data directory fallback (if HF datasets unavailable).
        spm_dir: SentencePiece model directory.
        src_lang: Source language (e.g., 'en').
        tgt_lang: Target language (e.g., 'de').
        vocab_size: SentencePiece vocab size.
        max_len: Max sequence length in tokens.
        batch_size: Approximate tokens per step (heuristic to set batch of sentences).
        num_epochs: Training epochs.
        lr: Base LR factor used by Noam scheduler.
        warmup_steps: Warmup steps for Noam.
        label_smoothing: Label smoothing epsilon [0,1).
        dropout: Dropout probability [0,1).
        d_model: Model hidden size (divisible by n_heads).
        n_heads: Number of attention heads.
        n_layers: Number of encoder/decoder layers.
        d_ff: Position-wise feed-forward hidden size.
        pos_encoding: Positional encoding type.
        weight_tying: Tie decoder embedding with generator.
        grad_clip: Gradient clipping norm (0 disables).
        fp16: Use automatic mixed precision when True.
        save_every: Save checkpoint every N steps.
        log_every: Log metrics every N steps.
        eval_every: Evaluate every N steps.
        num_workers: DataLoader workers.
        seed: Random seed.
        device: 'cuda' or 'cpu'.
        max_train_samples: Cap training samples (debug).
        max_eval_samples: Cap eval/test samples (debug).
        group: Ablation group tag.
    """

    run_name: str = "baseline_en-de"
    log_dir: str = "runs"
    save_dir: str = "checkpoints"
    data_dir: str = "data_wmt14"
    spm_dir: str = "spm_models"
    src_lang: str = "en"
    tgt_lang: str = "de"
    vocab_size: int = 37000
    max_len: int = 256
    batch_size: int = 8192  # 中文：按 token 规模估算的批大小（这里用于设定句子批大小的启发式）
    num_epochs: int = 10
    lr: float = 2.0
    warmup_steps: int = 4000
    label_smoothing: float = 0.1
    dropout: float = 0.1
    d_model: int = 512
    n_heads: int = 8
    n_layers: int = 6
    d_ff: int = 2048
    pos_encoding: Literal["sinusoidal", "learned"] = "sinusoidal"
    weight_tying: bool = True
    grad_clip: float = 1.0
    fp16: bool = True
    save_every: int = 5000
    log_every: int = 100
    eval_every: int = 5000
    num_workers: int = 4
    seed: int = 42
    device: str = "cuda"
    max_train_samples: Optional[int] = None
    max_eval_samples: Optional[int] = None
    group: str = "ablation"

    def ensure_dirs(self) -> None:
        """Ensure important directories exist."""
        # 中文：确保必要的输出目录存在
        Path(self.log_dir).mkdir(parents=True, exist_ok=True)
        Path(self.save_dir).mkdir(parents=True, exist_ok=True)
        Path(self.spm_dir).mkdir(parents=True, exist_ok=True)
```

## `main.py`

```python
# -*- coding: utf-8 -*-
"""Entry point for running ablation experiments.

This script parses CLI, builds config, and dispatches ablation groups.

Author: GPT-5 Thinking
"""

from __future__ import annotations

import argparse
import json
import sys
from pathlib import Path
from typing import Optional, Sequence

import torch
from loguru import logger

from config import ExperimentConfig
from training.ablation import AblationRunner
from training.trainer import run_sanity_checks
from utils.logging import setup_logging
from utils.seed import set_global_seed


def _parse_args(argv: Optional[Sequence[str]] = None) -> argparse.Namespace:
    """Parse command-line arguments.

    Args:
        argv: Optional argument list.

    Returns:
        Parsed namespace.
    """
    parser = argparse.ArgumentParser(
        description="Transformer Ablation Study on WMT14 (PyTorch + tensorboardX + loguru)"
    )
    parser.add_argument("--run_name", type=str, default="baseline_en-de")
    parser.add_argument("--group", type=str, default="baseline",
                        choices=["baseline", "heads", "dimensions", "dropout", "posenc"])
    parser.add_argument("--src_lang", type=str, default="en")
    parser.add_argument("--tgt_lang", type=str, default="de")
    parser.add_argument("--vocab_size", type=int, default=37000)
    parser.add_argument("--max_len", type=int, default=256)
    parser.add_argument("--batch_size", type=int, default=8192)
    parser.add_argument("--epochs", type=int, default=10)
    parser.add_argument("--lr", type=float, default=2.0)
    parser.add_argument("--warmup", type=int, default=4000)
    parser.add_argument("--label_smoothing", type=float, default=0.1)
    parser.add_argument("--dropout", type=float, default=0.1)
    parser.add_argument("--d_model", type=int, default=512)
    parser.add_argument("--n_heads", type=int, default=8)
    parser.add_argument("--n_layers", type=int, default=6)
    parser.add_argument("--d_ff", type=int, default=2048)
    parser.add_argument("--pos_enc", type=str, default="sinusoidal", choices=["sinusoidal", "learned"])
    parser.add_argument("--no_tying", action="store_true")
    parser.add_argument("--grad_clip", type=float, default=1.0)
    parser.add_argument("--no_fp16", action="store_true")
    parser.add_argument("--save_every", type=int, default=5000)
    parser.add_argument("--log_every", type=int, default=100)
    parser.add_argument("--eval_every", type=int, default=5000)
    parser.add_argument("--num_workers", type=int, default=4)
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu")
    parser.add_argument("--log_dir", type=str, default="runs")
    parser.add_argument("--save_dir", type=str, default="checkpoints")
    parser.add_argument("--data_dir", type=str, default="data_wmt14")
    parser.add_argument("--spm_dir", type=str, default="spm_models")
    parser.add_argument("--max_train_samples", type=int, default=None)
    parser.add_argument("--max_eval_samples", type=int, default=None)
    parser.add_argument("--sanity_check", action="store_true")
    return parser.parse_args(argv)


def _build_config(args: argparse.Namespace) -> ExperimentConfig:
    """Build ExperimentConfig from CLI args."""
    cfg = ExperimentConfig(
        run_name=args.run_name,
        log_dir=args.log_dir,
        save_dir=args.save_dir,
        data_dir=args.data_dir,
        spm_dir=args.spm_dir,
        src_lang=args.src_lang,
        tgt_lang=args.tgt_lang,
        vocab_size=args.vocab_size,
        max_len=args.max_len,
        batch_size=args.batch_size,
        num_epochs=args.epochs,
        lr=args.lr,
        warmup_steps=args.warmup,
        label_smoothing=args.label_smoothing,
        dropout=args.dropout,
        d_model=args.d_model,
        n_heads=args.n_heads,
        n_layers=args.n_layers,
        d_ff=args.d_ff,
        pos_encoding=args.pos_enc,
        weight_tying=not args.no_tying,
        grad_clip=args.grad_clip,
        fp16=not args.no_fp16,
        save_every=args.save_every,
        log_every=args.log_every,
        eval_every=args.eval_every,
        num_workers=args.num_workers,
        seed=args.seed,
        device=args.device,
        max_train_samples=args.max_train_samples,
        max_eval_samples=args.max_eval_samples,
        group=args.group,
    )
    return cfg


def main(argv: Optional[Sequence[str]] = None) -> None:
    """Main entry."""
    args = _parse_args(argv)
    setup_logging()  # 中文：设置 loguru 日志格式
    if args.sanity_check:
        run_sanity_checks()
        return

    cfg = _build_config(args)
    cfg.ensure_dirs()
    set_global_seed(cfg.seed)

    # 中文：打印配置，便于可追溯
    logger.info("Experiment config:\n" + json.dumps(cfg.__dict__, indent=2, ensure_ascii=False))

    # 中文：运行消融组
    runner = AblationRunner(cfg)
    results = runner.run_group(cfg.group)

    # 中文：汇总打印
    logger.info("==== Ablation Summary ====")
    for row in results:
        logger.info(
            f"[{row['run_name']}] h={row['n_heads']} d_model={row['d_model']} d_ff={row['d_ff']} "
            f"dropout={row['dropout']} pos={row['pos_encoding']} | "
            f"best_valid_BLEU={row['best_valid_bleu']:.2f} test_BLEU={row['test_bleu']:.2f}"
        )


if __name__ == "__main__":
    main()
```

---

# File category: Data

## `data/__init__.py`

```python
# -*- coding: utf-8 -*-
"""Data package exports."""

from .datamodule import WMT14DataModule, TranslationExample, DynamicBatchCollator

__all__ = [
    "WMT14DataModule",
    "TranslationExample",
    "DynamicBatchCollator",
]
```

## `data/datamodule.py`

```python
# -*- coding: utf-8 -*-
"""Data module for WMT14 translation tasks.

This module loads WMT14 via HuggingFace datasets (if available) or local TSV,
handles SentencePiece tokenization, builds PyTorch datasets and dataloaders.

Author: GPT-5 Thinking
"""

from __future__ import annotations

import os
from pathlib import Path
from typing import Dict, List, Optional, Sequence, Tuple

import torch
from loguru import logger
from torch.utils.data import DataLoader, Dataset

from config import ExperimentConfig

# 可选依赖：datasets、sentencepiece
try:
    from datasets import DatasetDict, load_dataset  # type: ignore
    HF_DATASETS_AVAILABLE = True
except Exception:
    HF_DATASETS_AVAILABLE = False

try:
    import sentencepiece as spm  # type: ignore
    SENTENCEPIECE_AVAILABLE = True
except Exception:
    SENTENCEPIECE_AVAILABLE = False


class SentencePieceTokenizer:
    """SentencePiece tokenizer with train/load/encode/decode.

    Note:
        Requires 'sentencepiece' package.

    Attributes:
        model_prefix: Path prefix for SPM files.
        vocab_size: Vocabulary size.
        pad_id/bos_id/eos_id/unk_id: Special ids.
    """

    def __init__(self, model_prefix: str, vocab_size: int = 37000) -> None:
        if not SENTENCEPIECE_AVAILABLE:
            raise RuntimeError("sentencepiece is required but not installed.")
        self.model_prefix = model_prefix
        self.vocab_size = vocab_size
        self._sp = spm.SentencePieceProcessor()  # type: ignore
        self.pad_id = 0
        self.bos_id = 1
        self.eos_id = 2
        self.unk_id = 3

    def train(self, input_files: List[str]) -> None:
        """Train SPM model."""
        # 中文：训练 SentencePiece 模型，生成 .model/.vocab
        args = {
            "input": ",".join(input_files),
            "model_prefix": self.model_prefix,
            "vocab_size": self.vocab_size,
            "character_coverage": 1.0,
            "model_type": "bpe",
            "pad_id": self.pad_id,
            "bos_id": self.bos_id,
            "eos_id": self.eos_id,
            "unk_id": self.unk_id,
        }
        spm.SentencePieceTrainer.Train(**args)  # type: ignore

    def load(self) -> None:
        """Load SPM model."""
        model_file = f"{self.model_prefix}.model"
        if not os.path.exists(model_file):
            raise FileNotFoundError(f"SPM model not found: {model_file}")
        self._sp.Load(model_file)  # type: ignore

    def encode(self, text: str, add_bos: bool = True, add_eos: bool = True) -> List[int]:
        """Encode text to ids."""
        ids = self._sp.EncodeAsIds(text)  # type: ignore
        if add_bos:
            ids = [self.bos_id] + ids
        if add_eos:
            ids = ids + [self.eos_id]
        return ids

    def decode(self, ids: List[int]) -> str:
        """Decode ids to text."""
        return self._sp.DecodeIds(ids)  # type: ignore

    @property
    def vocab_size_(self) -> int:
        """Return vocab size from SPM."""
        return self._sp.GetPieceSize()  # type: ignore


class TranslationExample(Dataset):
    """Dataset of tokenized parallel sentences."""

    def __init__(self, src: List[List[int]], tgt: List[List[int]], max_len: int) -> None:
        # 中文：存储分词后的源/目标序列
        self.src = src
        self.tgt = tgt
        self.max_len = max_len

    def __len__(self) -> int:
        return len(self.src)

    def __getitem__(self, idx: int) -> Tuple[List[int], List[int]]:
        # 中文：截断到 max_len，保证下游张量对齐
        s = self.src[idx][: self.max_len]
        t = self.tgt[idx][: self.max_len]
        return s, t


class DynamicBatchCollator:
    """Collator that pads sequences dynamically and creates masks."""

    def __init__(self, pad_id: int, device: torch.device) -> None:
        self.pad_id = pad_id
        self.device = device

    def _pad(self, seqs: List[List[int]]) -> Tuple[torch.Tensor, torch.Tensor]:
        """Pad variable-length sequences.

        Returns:
            Tensor with shape (B, T_max) and key_padding_mask (B, T_max).
        """
        # 中文：使用最大长度进行右侧 padding
        max_len = max(len(s) for s in seqs)
        bsz = len(seqs)
        out = torch.full((bsz, max_len), fill_value=self.pad_id, dtype=torch.long)
        for i, seq in enumerate(seqs):
            out[i, : len(seq)] = torch.tensor(seq, dtype=torch.long)
        pad_mask = (out == self.pad_id)
        return out, pad_mask

    def _subsequent_mask(self, size: int) -> torch.Tensor:
        """Create a causal mask (upper-triangular True)."""
        # 中文：上三角为 True，表示不可见的未来位
        attn_shape = (1, size, size)
        mask = torch.triu(torch.ones(attn_shape, dtype=torch.bool), diagonal=1)
        return mask

    def __call__(self, batch: List[Tuple[List[int], List[int]]]) -> Dict[str, torch.Tensor]:
        # 中文：组装 batch 并生成所有必要的掩码
        src_list, tgt_list = zip(*batch)
        src, src_pad = self._pad(list(src_list))
        tgt, tgt_pad = self._pad(list(tgt_list))

        # 中文：构造训练输入/标签（自回归位移）
        tgt_in = tgt[:, :-1].contiguous()
        tgt_out = tgt[:, 1:].contiguous()

        seq_len = tgt_in.size(1)
        subsequent = self._subsequent_mask(seq_len).to(self.device)

        return {
            "src": src.to(self.device),
            "tgt_in": tgt_in.to(self.device),
            "tgt_out": tgt_out.to(self.device),
            "src_key_padding_mask": src_pad.to(self.device),
            "tgt_key_padding_mask": tgt_pad[:, :-1].to(self.device),
            "tgt_mask": subsequent,
        }


class WMT14DataModule:
    """Data module that orchestrates WMT14 loading and tokenization."""

    def __init__(self, cfg: ExperimentConfig) -> None:
        self.cfg = cfg
        self.sp_src: Optional[SentencePieceTokenizer] = None
        self.sp_tgt: Optional[SentencePieceTokenizer] = None
        self.ds_train: Optional[TranslationExample] = None
        self.ds_valid: Optional[TranslationExample] = None
        self.ds_test: Optional[TranslationExample] = None

    def _load_hf(self) -> Tuple[List[str], List[str], List[str], List[str], List[str], List[str]]:
        """Load WMT14 from HuggingFace datasets."""
        if not HF_DATASETS_AVAILABLE:
            raise RuntimeError("datasets is not installed.")
        # 中文：尝试两种配置名（en-de 与 de-en），并做方向对齐
        attempts = [
            (f"{self.cfg.src_lang}-{self.cfg.tgt_lang}", self.cfg.src_lang, self.cfg.tgt_lang),
            (f"{self.cfg.tgt_lang}-{self.cfg.src_lang}", self.cfg.tgt_lang, self.cfg.src_lang),
        ]
        last_exc: Optional[Exception] = None
        for conf_name, src_key, tgt_key in attempts:
            try:
                dsd: DatasetDict = load_dataset("wmt14", conf_name)  # type: ignore
                def _pairs(split: str) -> Tuple[List[str], List[str]]:
                    xs: List[str] = []
                    ys: List[str] = []
                    for ex in dsd[split]:
                        t = ex["translation"]
                        xs.append(t[src_key])
                        ys.append(t[tgt_key])
                    return xs, ys

                train_x, train_y = _pairs("train")
                valid_x, valid_y = _pairs("validation") if "validation" in dsd else _pairs("test")
                test_x, test_y = _pairs("test")

                # 中文：若方向与当前 cfg 不一致，交换
                if src_key != self.cfg.src_lang:
                    train_x, train_y = train_y, train_x
                    valid_x, valid_y = valid_y, valid_x
                    test_x, test_y = test_y, test_x
                return train_x, train_y, valid_x, valid_y, test_x, test_y
            except Exception as e:  # noqa: BLE001
                last_exc = e
        raise RuntimeError(f"Failed to load WMT14 via datasets. Last error: {last_exc}")

    def _load_local(self, split: str) -> Tuple[List[str], List[str]]:
        """Load local TSV with 'src\\t tgt' per line."""
        f = Path(self.cfg.data_dir) / f"{split}.{self.cfg.src_lang}-{self.cfg.tgt_lang}.tsv"
        if not f.exists():
            raise FileNotFoundError(f"Local data file missing: {f}")
        xs, ys = [], []
        with open(f, "r", encoding="utf-8") as fh:
            for line in fh:
                line = line.strip()
                if not line:
                    continue
                pair = line.split("\t")
                if len(pair) != 2:
                    continue
                xs.append(pair[0])
                ys.append(pair[1])
        return xs, ys

    def _maybe_train_or_load_spm(self, src_corpus: List[str], tgt_corpus: List[str]) -> None:
        """Train or load SPM models for src/tgt."""
        Path(self.cfg.spm_dir).mkdir(parents=True, exist_ok=True)
        src_prefix = str(Path(self.cfg.spm_dir) / f"spm_{self.cfg.src_lang}")
        tgt_prefix = str(Path(self.cfg.spm_dir) / f"spm_{self.cfg.tgt_lang}")

        self.sp_src = SentencePieceTokenizer(src_prefix, self.cfg.vocab_size)
        self.sp_tgt = SentencePieceTokenizer(tgt_prefix, self.cfg.vocab_size)

        if not (os.path.exists(f"{src_prefix}.model") and os.path.exists(f"{tgt_prefix}.model")):
            logger.info("Training SentencePiece models for src/tgt ...")
            self.sp_src.train(src_corpus)
            self.sp_tgt.train(tgt_corpus)
        else:
            logger.info("Using existing SentencePiece models.")

        self.sp_src.load()
        self.sp_tgt.load()

    def prepare(self) -> None:
        """Prepare tokenizers and datasets."""
        logger.info("Preparing WMT14 data ...")
        # 中文：优先使用 HF datasets，加失败回退本地 TSV
        if HF_DATASETS_AVAILABLE:
            try:
                train_x, train_y, valid_x, valid_y, test_x, test_y = self._load_hf()
                logger.info("Loaded WMT14 from HuggingFace datasets.")
            except Exception as e:  # noqa: BLE001
                logger.warning(f"HuggingFace datasets failed: {e}. Falling back to local files.")
                train_x, train_y = self._load_local("train")
                valid_x, valid_y = self._load_local("valid")
                test_x, test_y = self._load_local("test")
        else:
            logger.warning("datasets not installed; using local TSV.")
            train_x, train_y = self._load_local("train")
            valid_x, valid_y = self._load_local("valid")
            test_x, test_y = self._load_local("test")

        # 中文：下采样以便快速调试
        if self.cfg.max_train_samples:
            train_x, train_y = train_x[: self.cfg.max_train_samples], train_y[: self.cfg.max_train_samples]
        if self.cfg.max_eval_samples:
            valid_x, valid_y = valid_x[: self.cfg.max_eval_samples], valid_y[: self.cfg.max_eval_samples]
            test_x, test_y = test_x[: self.cfg.max_eval_samples], test_y[: self.cfg.max_eval_samples]

        if not SENTENCEPIECE_AVAILABLE:
            raise RuntimeError("sentencepiece is required. Please install it.")

        # 中文：用训练集文本训练 SPM（写入临时文件以支持 SPM 训练 API）
        Path(self.cfg.spm_dir).mkdir(parents=True, exist_ok=True)
        src_tmp = Path(self.cfg.spm_dir) / f"tmp_src_{self.cfg.src_lang}.txt"
        tgt_tmp = Path(self.cfg.spm_dir) / f"tmp_tgt_{self.cfg.tgt_lang}.txt"
        with open(src_tmp, "w", encoding="utf-8") as fs:
            for s in train_x:
                fs.write(s.strip() + "\n")
        with open(tgt_tmp, "w", encoding="utf-8") as ft:
            for s in train_y:
                ft.write(s.strip() + "\n")
        self._maybe_train_or_load_spm([str(src_tmp)], [str(tgt_tmp)])
        try:
            src_tmp.unlink(missing_ok=True)
            tgt_tmp.unlink(missing_ok=True)
        except Exception:
            pass

        assert self.sp_src and self.sp_tgt

        def _encode(xs: List[str], ys: List[str]) -> Tuple[List[List[int]], List[List[int]]]:
            src_tok = [self.sp_src.encode(x) for x in xs]
            tgt_tok = [self.sp_tgt.encode(y) for y in ys]
            return src_tok, tgt_tok

        train_src_tok, train_tgt_tok = _encode(train_x, train_y)
        valid_src_tok, valid_tgt_tok = _encode(valid_x, valid_y)
        test_src_tok, test_tgt_tok = _encode(test_x, test_y)

        self.ds_train = TranslationExample(train_src_tok, train_tgt_tok, self.cfg.max_len)
        self.ds_valid = TranslationExample(valid_src_tok, valid_tgt_tok, self.cfg.max_len)
        self.ds_test = TranslationExample(test_src_tok, test_tgt_tok, self.cfg.max_len)

        logger.info(f"Prepared datasets: train={len(self.ds_train)} valid={len(self.ds_valid)} test={len(self.ds_test)}")

    def create_dataloaders(self) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """Create train/valid/test dataloaders."""
        assert self.ds_train and self.ds_valid and self.ds_test
        device = torch.device(self.cfg.device)
        collate = DynamicBatchCollator(pad_id=0, device=device)

        # 中文：此处用句子 batch；若要更精确按 token 控制，可改造成自定义批采样器
        train_loader = DataLoader(
            self.ds_train,
            batch_size=max(1, self.cfg.batch_size // 128),
            shuffle=True,
            num_workers=self.cfg.num_workers,
            collate_fn=collate,
            pin_memory=True,
            drop_last=True,
        )
        valid_loader = DataLoader(
            self.ds_valid,
            batch_size=max(1, self.cfg.batch_size // 256),
            shuffle=False,
            num_workers=self.cfg.num_workers,
            collate_fn=collate,
            pin_memory=True,
            drop_last=False,
        )
        test_loader = DataLoader(
            self.ds_test,
            batch_size=max(1, self.cfg.batch_size // 256),
            shuffle=False,
            num_workers=self.cfg.num_workers,
            collate_fn=collate,
            pin_memory=True,
            drop_last=False,
        )
        return train_loader, valid_loader, test_loader
```

---

# File category: Models

## `models/__init__.py`

```python
# -*- coding: utf-8 -*-
"""Model package exports."""

from .layers import SinusoidalPositionalEncoding, LearnedPositionalEncoding
from .losses import LabelSmoothingLoss
from .schedulers import NoamScheduler
from .transformer import TransformerSeq2Seq

__all__ = [
    "SinusoidalPositionalEncoding",
    "LearnedPositionalEncoding",
    "LabelSmoothingLoss",
    "NoamScheduler",
    "TransformerSeq2Seq",
]
```

## `models/layers.py`

```python
# -*- coding: utf-8 -*-
"""Transformer utility layers (positional encodings)."""

from __future__ import annotations

import math
import torch
import torch.nn as nn


class SinusoidalPositionalEncoding(nn.Module):
    """Sinusoidal positional encoding (non-trainable)."""

    def __init__(self, d_model: int, max_len: int = 10000) -> None:
        super().__init__()
        # 中文：预计算位置编码矩阵，注册为 buffer
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer("pe", pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Add positional encoding to embeddings.

        Args:
            x: Tensor of shape (B, T, D).

        Returns:
            Tensor with positional encodings added.
        """
        # 中文：根据序列长度切片位置编码并相加
        return x + self.pe[: x.size(1)].unsqueeze(0)


class LearnedPositionalEncoding(nn.Embedding):
    """Learned positional encoding."""

    def __init__(self, max_len: int, d_model: int) -> None:
        super().__init__(max_len, d_model)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Add learned positional embeddings.

        Args:
            x: Tensor of shape (B, T, D).

        Returns:
            Tensor with learned positional embeddings added.
        """
        # 中文：构造 [0..T-1] 位置索引并取嵌入
        bsz, seq_len, _ = x.size()
        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(bsz, -1)
        return x + super().forward(positions)
```

## `models/transformer.py`

```python
# -*- coding: utf-8 -*-
"""Transformer Seq2Seq model wrapping nn.Transformer."""

from __future__ import annotations

import math
import torch
import torch.nn as nn

from .layers import LearnedPositionalEncoding, SinusoidalPositionalEncoding


class TransformerSeq2Seq(nn.Module):
    """Transformer encoder-decoder with configurable PE and weight tying."""

    def __init__(
        self,
        vocab_src: int,
        vocab_tgt: int,
        d_model: int,
        n_heads: int,
        n_layers: int,
        d_ff: int,
        dropout: float,
        max_len: int,
        pos_encoding: str = "sinusoidal",
        weight_tying: bool = True,
    ) -> None:
        super().__init__()
        if d_model % n_heads != 0:
            raise ValueError("d_model must be divisible by n_heads.")
        if not (0.0 <= dropout < 1.0):
            raise ValueError("dropout must be in [0,1).")
        if pos_encoding not in ("sinusoidal", "learned"):
            raise ValueError("pos_encoding must be 'sinusoidal' or 'learned'.")

        # 中文：词嵌入与位置编码
        self.src_embed = nn.Embedding(vocab_src, d_model, padding_idx=0)
        self.tgt_embed = nn.Embedding(vocab_tgt, d_model, padding_idx=0)
        self.scale = math.sqrt(d_model)

        if pos_encoding == "sinusoidal":
            self.pos_enc = SinusoidalPositionalEncoding(d_model, max_len=max_len)
        else:
            self.pos_enc = LearnedPositionalEncoding(max_len=max_len, d_model=d_model)

        # 中文：核心 Transformer（batch_first 方便 (B,T,D)）
        self.transformer = nn.Transformer(
            d_model=d_model,
            nhead=n_heads,
            num_encoder_layers=n_layers,
            num_decoder_layers=n_layers,
            dim_feedforward=d_ff,
            dropout=dropout,
            batch_first=True,
            norm_first=False,
        )

        # 中文：输出生成线性层（可与 tgt_embed 权重共享）
        self.generator = nn.Linear(d_model, vocab_tgt, bias=False)
        if weight_tying:
            if self.tgt_embed.weight.size() != self.generator.weight.size():
                raise ValueError("Weight tying requires equal shapes of embedding and generator.")
            self.generator.weight = self.tgt_embed.weight

        self._reset_parameters()

    def _reset_parameters(self) -> None:
        # 中文：参数初始化（Xavier）
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)

    def encode(self, src_ids: torch.Tensor, src_key_padding_mask: torch.Tensor) -> torch.Tensor:
        """Encode source sequence to memory."""
        src = self.src_embed(src_ids) * self.scale
        src = self.pos_enc(src)
        memory = self.transformer.encoder(src, src_key_padding_mask=src_key_padding_mask)
        return memory

    def decode(
        self,
        tgt_in: torch.Tensor,
        memory: torch.Tensor,
        tgt_mask: torch.Tensor,
        tgt_key_padding_mask: torch.Tensor,
        memory_key_padding_mask: torch.Tensor,
    ) -> torch.Tensor:
        """Decode target sequence with encoder memory."""
        tgt = self.tgt_embed(tgt_in) * self.scale
        tgt = self.pos_enc(tgt)
        out = self.transformer.decoder(
            tgt=tgt,
            memory=memory,
            tgt_mask=tgt_mask[0],  # (T, T)
            tgt_key_padding_mask=tgt_key_padding_mask,
            memory_key_padding_mask=memory_key_padding_mask,
        )
        logits = self.generator(out)
        return logits

    def forward(
        self,
        src_ids: torch.Tensor,
        tgt_in: torch.Tensor,
        tgt_mask: torch.Tensor,
        src_key_padding_mask: torch.Tensor,
        tgt_key_padding_mask: torch.Tensor,
    ) -> torch.Tensor:
        """Forward pass for training."""
        memory = self.encode(src_ids, src_key_padding_mask)
        logits = self.decode(
            tgt_in=tgt_in,
            memory=memory,
            tgt_mask=tgt_mask,
            tgt_key_padding_mask=tgt_key_padding_mask,
            memory_key_padding_mask=src_key_padding_mask,
        )
        return logits

    @torch.no_grad()
    def greedy_decode(
        self,
        src_ids: torch.Tensor,
        src_key_padding_mask: torch.Tensor,
        max_len: int,
        bos_id: int,
        eos_id: int,
    ) -> torch.Tensor:
        """Greedy decoding loop."""
        device = src_ids.device
        memory = self.encode(src_ids, src_key_padding_mask)
        bsz = src_ids.size(0)
        ys = torch.full((bsz, 1), bos_id, dtype=torch.long, device=device)
        finished = torch.zeros(bsz, dtype=torch.bool, device=device)

        for _ in range(max_len - 1):
            t = ys.size(1)
            causal = torch.triu(torch.ones((1, t, t), dtype=torch.bool, device=device), diagonal=1)
            logits = self.decode(
                tgt_in=ys,
                memory=memory,
                tgt_mask=causal,
                tgt_key_padding_mask=torch.zeros((bsz, t), dtype=torch.bool, device=device),
                memory_key_padding_mask=src_key_padding_mask,
            )
            next_tok = torch.argmax(logits[:, -1, :], dim=-1)
            ys = torch.cat([ys, next_tok.unsqueeze(1)], dim=1)
            finished = finished | (next_tok == eos_id)
            if torch.all(finished):
                break
        return ys
```

## `models/losses.py`

```python
# -*- coding: utf-8 -*-
"""Loss functions (Label Smoothing)."""

from __future__ import annotations

import torch
import torch.nn as nn
import torch.nn.functional as F


class LabelSmoothingLoss(nn.Module):
    """KL-based label smoothing loss for token-level classification."""

    def __init__(self, vocab_size: int, smoothing: float, ignore_index: int = 0) -> None:
        super().__init__()
        if not (0.0 <= smoothing < 1.0):
            raise ValueError("label_smoothing must be in [0,1).")
        # 中文：忽略 PAD；设置平滑概率
        self.ignore_index = ignore_index
        self.smoothing = smoothing
        self.confidence = 1.0 - smoothing
        self.vocab_size = vocab_size

    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """Compute smoothed cross-entropy.

        Args:
            pred: Logits (N, V).
            target: Target ids (N,).

        Returns:
            Scalar loss tensor.
        """
        mask = (target != self.ignore_index)
        pred = pred[mask]
        target = target[mask]
        if pred.numel() == 0:
            return pred.new_tensor(0.0)

        with torch.no_grad():
            true_dist = torch.full_like(pred, fill_value=self.smoothing / (self.vocab_size - 1))
            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)
        loss = F.kl_div(F.log_softmax(pred, dim=-1), true_dist, reduction="batchmean")
        return loss
```

## `models/schedulers.py`

```python
# -*- coding: utf-8 -*-
"""Learning rate schedulers (Noam)."""

from __future__ import annotations

from typing import List

import torch


class NoamScheduler(torch.optim.lr_scheduler._LRScheduler):
    """Noam learning rate scheduler from Vaswani et al. 2017."""

    def __init__(self, optimizer: torch.optim.Optimizer, d_model: int, warmup_steps: int, last_epoch: int = -1) -> None:
        self.d_model = d_model
        self.warmup_steps = warmup_steps
        super().__init__(optimizer, last_epoch)

    def get_lr(self) -> List[float]:  # type: ignore[override]
        # 中文：按 step 计算缩放，前 warmup 线性上升，后续 1/sqrt(step) 衰减
        step = max(1, self.last_epoch + 1)
        scale = (self.d_model ** -0.5) * min(step ** -0.5, step * (self.warmup_steps ** -1.5))
        return [base_lr * scale for base_lr in self.base_lrs]
```

---

# File category: Training

## `training/__init__.py`

```python
# -*- coding: utf-8 -*-
"""Training package exports."""

from .trainer import Trainer, run_sanity_checks
from .ablation import AblationRunner

__all__ = ["Trainer", "run_sanity_checks", "AblationRunner"]
```

## `training/trainer.py`

```python
# -*- coding: utf-8 -*-
"""Trainer: training loop, evaluation, logging, checkpointing."""

from __future__ import annotations

import dataclasses
import json
import time
from pathlib import Path
from typing import Any, Dict, List, Tuple

import sacrebleu  # type: ignore
import torch
import torch.nn as nn
from loguru import logger
from tensorboardX import SummaryWriter
from torch.utils.data import DataLoader

from config import ExperimentConfig
from data.datamodule import WMT14DataModule
from models.losses import LabelSmoothingLoss
from models.schedulers import NoamScheduler
from models.transformer import TransformerSeq2Seq
from utils.io import ensure_dir


class Trainer:
    """High-level trainer coordinating model, optimizer, scheduler, and logs."""

    def __init__(self, cfg: ExperimentConfig, data: WMT14DataModule) -> None:
        self.cfg = cfg
        self.data = data
        self.device = torch.device(cfg.device)
        self.writer: SummaryWriter | None = None
        self.global_step = 0

        # 中文：训练输出目录
        self.run_dir = ensure_dir(Path(cfg.log_dir) / cfg.group / cfg.run_name, clear_if_exists=False)
        self.ckpt_dir = ensure_dir(Path(cfg.save_dir) / cfg.group / cfg.run_name, clear_if_exists=False)

    def _build(self) -> Tuple[TransformerSeq2Seq, torch.optim.Optimizer, NoamScheduler, LabelSmoothingLoss]:
        """Build model, optimizer, scheduler, and criterion."""
        assert self.data.sp_src and self.data.sp_tgt
        vocab_src = self.data.sp_src.vocab_size_
        vocab_tgt = self.data.sp_tgt.vocab_size_
        logger.info(f"Vocab sizes -> src: {vocab_src} | tgt: {vocab_tgt}")

        model = TransformerSeq2Seq(
            vocab_src=vocab_src,
            vocab_tgt=vocab_tgt,
            d_model=self.cfg.d_model,
            n_heads=self.cfg.n_heads,
            n_layers=self.cfg.n_layers,
            d_ff=self.cfg.d_ff,
            dropout=self.cfg.dropout,
            max_len=self.cfg.max_len,
            pos_encoding=self.cfg.pos_encoding,
            weight_tying=self.cfg.weight_tying,
        ).to(self.device)

        optimizer = torch.optim.Adam(model.parameters(), lr=self.cfg.lr, betas=(0.9, 0.98), eps=1e-9)
        scheduler = NoamScheduler(optimizer, d_model=self.cfg.d_model, warmup_steps=self.cfg.warmup_steps)
        criterion = LabelSmoothingLoss(vocab_size=vocab_tgt, smoothing=self.cfg.label_smoothing, ignore_index=0)

        return model, optimizer, scheduler, criterion

    def _evaluate_bleu(self, model: TransformerSeq2Seq, loader: DataLoader) -> float:
        """Compute SacreBLEU on a dataloader (greedy decoding)."""
        assert self.data.sp_tgt and self.data.sp_src
        refs: List[str] = []
        hyps: List[str] = []
        model.eval()
        with torch.no_grad():
            for batch in loader:
                src = batch["src"]
                src_kpm = batch["src_key_padding_mask"]

                # 中文：贪心解码
                gen = model.greedy_decode(
                    src_ids=src,
                    src_key_padding_mask=src_kpm,
                    max_len=self.cfg.max_len,
                    bos_id=self.data.sp_tgt.bos_id,
                    eos_id=self.data.sp_tgt.eos_id,
                )

                # 中文：收集参考与假设文本（去除 BOS、PAD，截断至 EOS）
                tgt_out = batch["tgt_out"].cpu().tolist()
                gen_out = gen.cpu().tolist()
                for ref_ids, hyp_ids in zip(tgt_out, gen_out):
                    ref_ids_clean = [x for x in ref_ids if x != 0]
                    hyp_ids_clean = [x for x in hyp_ids if x != 0]
                    if hyp_ids_clean and hyp_ids_clean[0] == self.data.sp_tgt.bos_id:
                        hyp_ids_clean = hyp_ids_clean[1:]
                    if self.data.sp_tgt.eos_id in hyp_ids_clean:
                        e = hyp_ids_clean.index(self.data.sp_tgt.eos_id)
                        hyp_ids_clean = hyp_ids_clean[: e + 1]
                    refs.append(self.data.sp_tgt.decode(ref_ids_clean))
                    hyps.append(self.data.sp_tgt.decode(hyp_ids_clean))

        bleu = sacrebleu.corpus_bleu(hyps, [refs]).score  # type: ignore
        return float(bleu)

    def train(self) -> Dict[str, Any]:
        """Run training and periodic evaluation."""
        train_loader, valid_loader, test_loader = self.data.create_dataloaders()
        model, optimizer, scheduler, criterion = self._build()

        # 中文：TensorBoard 写入器
        self.writer = SummaryWriter(logdir=str(self.run_dir))

        # 中文：混合精度
        scaler = torch.cuda.amp.GradScaler(enabled=self.cfg.fp16)

        logger.info(f"Start training: epochs={self.cfg.num_epochs} device={self.cfg.device}")
        best_valid_bleu = -1.0
        best_ckpt_path: str | None = None
        t0 = time.time()

        for epoch in range(1, self.cfg.num_epochs + 1):
            model.train()
            epoch_loss = 0.0
            step_in_epoch = 0

            for batch in train_loader:
                self.global_step += 1
                step_in_epoch += 1

                src = batch["src"]
                tgt_in = batch["tgt_in"]
                tgt_out = batch["tgt_out"]
                src_kpm = batch["src_key_padding_mask"]
                tgt_kpm = batch["tgt_key_padding_mask"]
                tgt_mask = batch["tgt_mask"]

                optimizer.zero_grad(set_to_none=True)

                with torch.cuda.amp.autocast(enabled=self.cfg.fp16):
                    logits = model(
                        src_ids=src,
                        tgt_in=tgt_in,
                        tgt_mask=tgt_mask,
                        src_key_padding_mask=src_kpm,
                        tgt_key_padding_mask=tgt_kpm,
                    )
                    bsz, tlen, vocab = logits.shape
                    loss = criterion(logits.reshape(bsz * tlen, vocab), tgt_out.reshape(bsz * tlen))

                scaler.scale(loss).backward()
                if self.cfg.grad_clip and self.cfg.grad_clip > 0.0:
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), self.cfg.grad_clip)
                scaler.step(optimizer)
                scaler.update()
                scheduler.step()

                epoch_loss += loss.item()

                if self.global_step % self.cfg.log_every == 0:
                    lr = scheduler.get_last_lr()[0]
                    avg_loss = epoch_loss / step_in_epoch
                    logger.info(f"step={self.global_step} epoch={epoch} loss={loss.item():.4f} avg={avg_loss:.4f} lr={lr:.6e}")
                    assert self.writer
                    self.writer.add_scalar("train/loss", loss.item(), self.global_step)
                    self.writer.add_scalar("train/lr", lr, self.global_step)

                if self.global_step % self.cfg.eval_every == 0:
                    valid_bleu = self._evaluate_bleu(model, valid_loader)
                    logger.info(f"[Eval] step={self.global_step} valid_BLEU={valid_bleu:.2f}")
                    assert self.writer
                    self.writer.add_scalar("valid/BLEU", valid_bleu, self.global_step)
                    if valid_bleu > best_valid_bleu:
                        best_valid_bleu = valid_bleu
                        best_ckpt_path = str(Path(self.ckpt_dir) / f"best_step{self.global_step}.pt")
                        torch.save(
                            {
                                "model": model.state_dict(),
                                "optimizer": optimizer.state_dict(),
                                "scheduler": scheduler.state_dict(),
                                "config": dataclasses.asdict(self.cfg),
                                "global_step": self.global_step,
                            },
                            best_ckpt_path,
                        )
                        logger.info(f"Saved best checkpoint to {best_ckpt_path}")

                if self.global_step % self.cfg.save_every == 0:
                    ckpt_path = str(Path(self.ckpt_dir) / f"ckpt_step{self.global_step}.pt")
                    torch.save({"model": model.state_dict(), "global_step": self.global_step}, ckpt_path)
                    logger.info(f"Saved checkpoint to {ckpt_path}")

            valid_bleu = self._evaluate_bleu(model, valid_loader)
            logger.info(f"[EpochEnd] epoch={epoch} valid_BLEU={valid_bleu:.2f}")
            assert self.writer
            self.writer.add_scalar("valid/BLEU_epoch", valid_bleu, epoch)
            if valid_bleu > best_valid_bleu:
                best_valid_bleu = valid_bleu
                best_ckpt_path = str(Path(self.ckpt_dir) / f"best_epoch{epoch}.pt")
                torch.save(
                    {
                        "model": model.state_dict(),
                        "optimizer": optimizer.state_dict(),
                        "scheduler": scheduler.state_dict(),
                        "config": dataclasses.asdict(self.cfg),
                        "global_step": self.global_step,
                    },
                    best_ckpt_path,
                )
                logger.info(f"Saved best checkpoint to {best_ckpt_path}")

        elapsed_h = (time.time() - t0) / 3600.0
        logger.info(f"Training finished in {elapsed_h:.2f}h. Best valid BLEU={best_valid_bleu:.2f}")

        # 中文：用最佳权重在测试集上评估
        test_bleu = 0.0
        if best_ckpt_path and Path(best_ckpt_path).exists():
            logger.info(f"Loading best checkpoint: {best_ckpt_path}")
            ckpt = torch.load(best_ckpt_path, map_location=self.device)
            model.load_state_dict(ckpt["model"])
            test_bleu = self._evaluate_bleu(model, test_loader)
            logger.info(f"[Test] BLEU={test_bleu:.2f}")
            assert self.writer
            self.writer.add_scalar("test/BLEU", test_bleu, self.global_step)

        if self.writer:
            self.writer.close()
        return {"best_valid_bleu": best_valid_bleu, "test_bleu": test_bleu}


def run_sanity_checks() -> None:
    """Run quick forward/decoding sanity checks."""
    logger.info("Running sanity checks ...")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = TransformerSeq2Seq(
        vocab_src=100,
        vocab_tgt=100,
        d_model=256,
        n_heads=4,
        n_layers=2,
        d_ff=1024,
        dropout=0.1,
        max_len=64,
        pos_encoding="sinusoidal",
        weight_tying=False,
    ).to(device)

    bsz, s_len, t_len = 2, 10, 12
    src = torch.randint(4, 100, (bsz, s_len), device=device)
    tgt_in = torch.randint(4, 100, (bsz, t_len), device=device)
    src_kpm = torch.zeros((bsz, s_len), dtype=torch.bool, device=device)
    tgt_kpm = torch.zeros((bsz, t_len), dtype=torch.bool, device=device)
    tgt_mask = torch.triu(torch.ones((1, t_len, t_len), dtype=torch.bool, device=device), diagonal=1)

    logits = model(src_ids=src, tgt_in=tgt_in, tgt_mask=tgt_mask, src_key_padding_mask=src_kpm, tgt_key_padding_mask=tgt_kpm)
    assert logits.shape == (bsz, t_len, 100), f"Unexpected logits shape: {logits.shape}"

    gen = model.greedy_decode(src_ids=src, src_key_padding_mask=src_kpm, max_len=20, bos_id=1, eos_id=2)
    assert gen.shape[0] == bsz and gen.shape[1] >= 1
    logger.info("Sanity checks passed ✅")
```

## `training/ablation.py`

```python
# -*- coding: utf-8 -*-
"""Ablation runner to orchestrate multiple experiments."""

from __future__ import annotations

import dataclasses
import json
import time
from pathlib import Path
from typing import Any, Dict, List, Literal

from loguru import logger

from config import ExperimentConfig
from data.datamodule import WMT14DataModule
from training.trainer import Trainer
from utils.io import ensure_dir
from utils.seed import set_global_seed


class AblationRunner:
    """Run ablation groups: heads, dimensions, dropout, posenc, baseline."""

    def __init__(self, base_cfg: ExperimentConfig) -> None:
        self.base_cfg = base_cfg
        self.results: List[Dict[str, Any]] = []

    def _run_single(self, cfg: ExperimentConfig) -> Dict[str, Any]:
        """Run a single experiment."""
        set_global_seed(cfg.seed)
        logger.info(f"Running experiment: {cfg.run_name} | group={cfg.group}")
        logger.info(json.dumps(dataclasses.asdict(cfg), indent=2, ensure_ascii=False))

        data = WMT14DataModule(cfg)
        data.prepare()

        trainer = Trainer(cfg, data)
        result = trainer.train()
        row = {
            "run_name": cfg.run_name,
            "group": cfg.group,
            "d_model": cfg.d_model,
            "d_ff": cfg.d_ff,
            "n_heads": cfg.n_heads,
            "dropout": cfg.dropout,
            "pos_encoding": cfg.pos_encoding,
            "best_valid_bleu": result["best_valid_bleu"],
            "test_bleu": result["test_bleu"],
        }
        self.results.append(row)

        out_dir = ensure_dir(Path(cfg.log_dir) / cfg.group)
        with open(out_dir / f"summary_{int(time.time())}.json", "w", encoding="utf-8") as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        return row

    def run_group(self, group: Literal["heads", "dimensions", "dropout", "posenc", "baseline"]) -> List[Dict[str, Any]]:
        """Create configs for a group and run them."""
        base = dataclasses.replace(self.base_cfg)
        cfgs: List[ExperimentConfig] = []

        if group == "baseline":
            cfgs = [dataclasses.replace(base, run_name=f"baseline_{base.src_lang}-{base.tgt_lang}")]
        elif group == "heads":
            for h in [1, 4, 8, 16]:
                cfgs.append(dataclasses.replace(base, n_heads=h, run_name=f"heads_h{h}_{base.src_lang}-{base.tgt_lang}"))
        elif group == "dimensions":
            for dm, df in [(256, 1024), (512, 2048), (1024, 4096)]:
                heads = 8 if dm % 8 == 0 else 4
                cfgs.append(dataclasses.replace(base, d_model=dm, d_ff=df, n_heads=heads, run_name=f"dim_dm{dm}_df{df}_{base.src_lang}-{base.tgt_lang}"))
        elif group == "dropout":
            for p in [0.0, 0.1, 0.2, 0.3]:
                cfgs.append(dataclasses.replace(base, dropout=p, run_name=f"dropout_p{str(p).replace('.','_')}_{base.src_lang}-{base.tgt_lang}"))
        elif group == "posenc":
            for t in ["sinusoidal", "learned"]:
                cfgs.append(dataclasses.replace(base, pos_encoding=t, run_name=f"posenc_{t}_{base.src_lang}-{base.tgt_lang}"))
        else:
            raise ValueError(f"Unknown group: {group}")

        results: List[Dict[str, Any]] = []
        for cfg in cfgs:
            results.append(self._run_single(cfg))
        return results
```

---

# File category: Utils

## `utils/__init__.py`

```python
# -*- coding: utf-8 -*-
"""Utilities package exports."""

from .io import ensure_dir
from .logging import setup_logging
from .seed import set_global_seed

__all__ = ["ensure_dir", "setup_logging", "set_global_seed"]
```

## `utils/logging.py`

```python
# -*- coding: utf-8 -*-
"""Logging setup using loguru."""

from __future__ import annotations

import sys
from loguru import logger


def setup_logging(level: str = "INFO") -> None:
    """Setup loguru logging format."""
    # 中文：配置统一日志样式
    logger.remove()
    logger.add(
        sys.stderr,
        level=level,
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level}</level> | <cyan>{message}</cyan>",
    )
```

## `utils/seed.py`

```python
# -*- coding: utf-8 -*-
"""Global random seed helpers."""

from __future__ import annotations

import random

import numpy as np
import torch


def set_global_seed(seed: int) -> None:
    """Set global seeds for reproducibility."""
    # 中文：固定随机种子，保证可复现
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
```

## `utils/io.py`

```python
# -*- coding: utf-8 -*-
"""IO helpers."""

from __future__ import annotations

import shutil
from pathlib import Path


def ensure_dir(path: str | Path, clear_if_exists: bool = False) -> Path:
    """Ensure directory exists.

    Args:
        path: Directory path.
        clear_if_exists: Remove and recreate if already exists.

    Returns:
        The created/existing Path.
    """
    # 中文：保证目录存在；必要时清空重建
    p = Path(path)
    if p.exists() and clear_if_exists:
        shutil.rmtree(p)
    p.mkdir(parents=True, exist_ok=True)
    return p
```

---

# File category: Scripts

## `scripts/run_sanity.py`

```python
# -*- coding: utf-8 -*-
"""Run quick sanity checks without training."""

from __future__ import annotations

from training.trainer import run_sanity_checks

if __name__ == "__main__":
    run_sanity_checks()
```

---

## 使用方式（快速参考）

* 安装依赖（建议虚拟环境）：

  ```bash
  pip install -r requirements.txt
  ```

* **Sanity Check（不训练）**：

  ```bash
  python scripts/run_sanity.py
  # 或
  python main.py --sanity_check
  ```

* **基线实验（英->德）**：

  ```bash
  python main.py --group baseline --src_lang en --tgt_lang de --run_name baseline_en-de
  ```

* **多头数消融**：

  ```bash
  python main.py --group heads --epochs 10
  ```

* **维度消融**：

  ```bash
  python main.py --group dimensions --epochs 10
  ```

* **Dropout 消融**：

  ```bash
  python main.py --group dropout --epochs 10
  ```

* **位置编码消融**：

  ```bash
  python main.py --group posenc --epochs 10
  ```

> 备注：首次运行建议带 `--max_train_samples 10000 --max_eval_samples 1000` 小规模调通；正式训练需要较多 GPU/时间。TensorBoard 可用：
>
> ```bash
> tensorboard --logdir runs
> ```

---

# ✅ 自检（Self-recheck）与改进点（Improvements）

我对关键逻辑做了逐条复核，并基于上个版本进一步改进：

1. **掩码与形状统一**

   * 训练：`tgt_mask` 形状 (1, T, T) → `decoder` 内部取 `[0]` 成 (T, T)；`*_key_padding_mask` 使用 `True=PAD` 语义，严格符合 `nn.Transformer`。
   * 解码：逐步构造因果掩码，`memory_key_padding_mask` 始终沿用源端 PAD 掩码。

2. **严密的边界检查**

   * `d_model % n_heads == 0`、`0 ≤ dropout < 1`、`0 ≤ smoothing < 1` 全部在构造期校验。
   * 权重共享前检查 `tgt_embed.weight` 与 `generator.weight` 尺寸一致。

3. **Label Smoothing 的 PAD 处理**

   * 明确用 `ignore_index=0` 屏蔽 PAD 的 loss 计算（先 mask 再 KLDiv），避免梯度污染。

4. **Noam 调度实现**

   * 使用 `(d_model^-0.5) * min(step^-0.5, step*warmup^-1.5)`，步数从 1 起，避免 0 除。

5. **SentencePiece 训练/加载**

   * 若不存在则训练；存在直接加载；临时文件使用后立即清理，防止脏数据残留。

6. **数据加载回退机制**

   * 先尝试 HF datasets 的 `wmt14` 多配置名；失败自动回退本地 TSV（`data_wmt14/train.en-de.tsv` 等）。

7. **Mixed Precision 与梯度裁剪顺序**

   * `scaler.unscale_` 后再裁剪，保证数值稳定性；优化器 step 与 scheduler step 顺序正确。

8. **Sanity Check 脚本**

   * 单元级检查前向形状与解码形状，快速发现配置/维度错误。

9. **工业可观测性**

   * loguru + tensorboardX：训练损失、学习率、BLEU（按 step/epoch）可视化；最佳 checkpoint 自动保存并回测测试集 BLEU。

10. **可扩展的消融框架**

    * `AblationRunner` 在单入口 `run_group` 下拼装多配置，汇总结果到 JSON，便于横向比较与追溯。

> 结论：当前版本在**可读性、鲁棒性、工业可观测性**方面进一步强化，逻辑闭环（数据→模型→训练→评估→消融→汇总）完整，适合直接用于研究/工程环境开展论文 Part 7 消融复现与扩展。需要我再加 **Beam Search、梯度累积、DDP 多卡** 或 **长序列注意力（如Linformer/Performer）** 的可插拔实现吗？我可以继续把这套模板扩展成一键多卡、可插拔注意力的实验平台。
