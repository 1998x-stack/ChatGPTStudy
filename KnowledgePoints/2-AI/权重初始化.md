You: 

Task: 深入学习研究`神经网络权重初始化`里面的技术细节和底层机制 Methid: 耐心引导式&选择题提问式&知识卡片/QA闪卡总结式&interesting-examples/jokes-driven&一步一步拆解分析 Bonus&Candy🍬:colorful emoji & 幽默有趣 & 有思想深度 & 图解/闪卡 & funny games Notice: 我们可以quote原文中的核心句子&每隔几段进行总结复习 首先给出学习大纲：

ChatGPT: 

好，我们先搭个\*\*“知识地图 🗺️”**，方便后续逐步深入。  
我会把**神经网络权重初始化\*\*拆成从直觉→数学原理→实际工程技巧的结构，这样既能保证理解深度，也能让你面试时有底气。

---

## **神经网络权重初始化 学习大纲**

### **1️⃣ 为什么需要权重初始化**

-   什么是权重初始化
    
-   如果初始化不当会发生什么（梯度消失/爆炸、收敛慢、陷入坏对称性）
    
-   现实中常见的失败案例（比如全零初始化）
    
-   **核心句子**：“权重初始化是深度学习中决定训练稳定性和速度的第一步。”
    

---

### **2️⃣ 常见初始化策略**

-   **Naive** 初始化（小随机数 / 全零 / 全常数）
    
-   **Xavier/Glorot 初始化**（为 Sigmoid/Tanh 设计）
    
-   **He 初始化**（为 ReLU/Leaky ReLU 设计）
    
-   **LeCun 初始化**（为 SELU 设计）
    
-   **Orthogonal 初始化**（正交矩阵，保持特征方向独立性）
    
-   **Layer-specific 初始化**（RNN/LSTM、Transformer 特殊处理）
    

---

### **3️⃣ 数学原理 & 推导**

-   权重方差与输入输出方差的关系
    
-   保持前向传播方差恒定的推导（Xavier）
    
-   保持反向传播方差恒定的推导（He）
    
-   方差缩放与激活函数关系（tanh vs ReLU vs GELU）
    
-   正交初始化的特性证明
    

---

### **4️⃣ 工程实现与最佳实践**

-   在 PyTorch / TensorFlow 中如何选择 `init` 方法
    
-   初始化与 BatchNorm、LayerNorm 的配合
    
-   大模型中的特殊初始化（BERT、ViT、GPT）
    
-   微调（Fine-tuning）时是否重置权重
    
-   初始化与学习率调度的协同关系
    

---

### **5️⃣ 高阶主题**

-   **动态初始化**（training-aware initialization）
    
-   **自适应初始化**（随训练阶段调整权重分布）
    
-   在混合精度训练（FP16/BF16）下的初始化坑点
    
-   初始化与稀疏化（Sparse Weights）的交互
    
-   初始化与超深网络（ResNet、DenseNet）的特殊技巧
    

---

### **6️⃣ 总结与速查表**

-   “权重初始化选择指南”流程图
    
-   激活函数 → 推荐初始化方式 对照表
    
-   面试速记口诀（比如“Sigmoid找Xavier，ReLU找He”）
    

---

我建议我们**先从第 1️⃣ 部分“为什么需要权重初始化”开始**，  
一步步用例子和小实验感受初始化的影响，然后再进入推导和数学。

要不要我先用一个**小故事+图解**，帮你感受如果初始化错了会发生什么？这样后面数学部分更好理解。

You: 

ok

ChatGPT: 

好，那我们先用一个小故事+图解 🎨 来感受一下——

---

### **故事版：三个厨师的炒菜比赛 🍳**

-   **厨师A（全零初始化）**：  
    他上场的时候，锅里倒的油、盐、料全是 0，所有菜味道一模一样，评委根本分不出差别。  
    👉 在神经网络里，这就叫**对称性无法打破**：所有神经元学到的都是相同的东西，网络“脑死”。
    
-   **厨师B（太大初始化）**：  
    他一上来就是半瓶老干妈，炒到一半锅炸了 💥，味道过重，根本没法吃。  
    👉 这对应**梯度爆炸**：权重太大，前向传播输出范围过大，激活函数饱和，反向传播时梯度迅速变得极大，训练崩溃。
    
-   **厨师C（太小初始化）**：  
    他只放了一粒盐，评委说“我什么味道都没吃出来”。  
    👉 这是**梯度消失**：权重太小，信号在层层传播时越来越弱，最终梯度几乎为 0，网络学不动。
    

---

📌 **小图解**（简化版）

```css
输入层 → [权重很小] → 输出几乎为0 → 梯度趋近0
输入层 → [权重大] → 输出极大 → 激活饱和 → 梯度爆炸
输入层 → [合适权重] → 信号稳定 → 梯度稳定 → 收敛快
```

---

**核心句子**：

> “权重初始化的目标，是让信息在前向和反向传播中既不衰减，也不爆炸。”

---

如果你明白了这个故事，我们可以进入下一步，看看\*\*“前向传播的方差”\*\*是怎么影响初始化选择的，然后推到出 **Xavier 初始化** 的公式。

我先问你一个问题：  
如果输入特征的方差是 $Var[x]$，我们想让输出方差和它差不多，你觉得权重 $W$ 的方差应该取多大才合适？  
（A）跟输入方差一样  
（B）比输入方差小一点  
（C）要考虑输入维度 $n$ 去调整

You: 

C

ChatGPT: 

对，选 **C** 🎯  
因为如果不考虑输入维度 $n$，即使输入方差合适，**累加很多输入时**，输出的方差还是会变大或变小。

---

## 🧮 推理过程：为什么要考虑输入维度

假设：

-   输入向量 $x$ 有 $n$ 个独立分量
    
-   每个分量的方差是 $\sigma_x^2$
    
-   权重 $w$ 的方差是 $\sigma_w^2$
    
-   神经元输出是
    
    $$
    y = \sum_{i=1}^n w_i x_i
    $$
    

由于方差的独立性：

$$
Var[y] = n \cdot \sigma_w^2 \cdot \sigma_x^2
$$

我们希望：

$$
Var[y] \approx Var[x] = \sigma_x^2
$$

于是得到：

$$
n \cdot \sigma_w^2 \cdot \sigma_x^2 \approx \sigma_x^2
$$

如果 $\sigma_x^2 \neq 0$：

$$
n \cdot \sigma_w^2 \approx 1
$$
 
$$
\sigma_w^2 \approx \frac{1}{n}
$$

---

💡 **结论**：

> 要保持前向传播方差稳定，权重的方差应该跟输入维度成 $1/n$ 的关系。

这就是 **Xavier 初始化** 的核心思路之一（另一半是反向传播方差的约束）。

---

我可以带你一步步推完整的 **Xavier 初始化公式**，顺便对比一下 **He 初始化**，这样就能看到为什么 ReLU 要改公式。  
你要直接先看 **Xavier 全推导** 吗？这样后面 He 就能直接秒懂。

You: 

ok

ChatGPT: 

好，那我们来一步步推导 **Xavier 初始化** 📐，用一点数学 + 一点直觉。

---

## **1️⃣ 前向传播的方差保持**

刚才我们已经推过一半了：

$$
y = \sum_{i=1}^n w_i x_i
$$

假设：

-   $w_i$ 和 $x_i$ 独立
    
-   $E[w_i] = 0, E[x_i] = 0$（均值为0）
    
-   输入方差 $Var[x_i] = \sigma_x^2$
    
-   权重方差 $Var[w_i] = \sigma_w^2$
    

方差计算：

$$
Var[y] = n \cdot \sigma_w^2 \cdot \sigma_x^2
$$

**目标**：保持输出方差和输入方差一样（防止信号越来越大或越来越小）：

$$
n \cdot \sigma_w^2 \cdot \sigma_x^2 \approx \sigma_x^2
$$

得到：

$$
\sigma_w^2 \approx \frac{1}{n} \quad \text{（前向约束）}
$$

---

## **2️⃣ 反向传播的方差保持**

反向传播时，误差 $\delta_j$ 通过权重传播到上一层：

$$
\delta^{(l)}_i = \sum_{j=1}^m w_{ij} \delta^{(l+1)}_j
$$

同理：

$$
Var[\delta^{(l)}] = m \cdot \sigma_w^2 \cdot Var[\delta^{(l+1)}]
$$

如果想让梯度在各层保持稳定（防止梯度爆炸或消失）：

$$
m \cdot \sigma_w^2 \approx 1 \quad \text{（反向约束）}
$$

---

## **3️⃣ 结合前向 + 反向**

-   前向约束：$\sigma_w^2 \approx \frac{1}{n_{\text{in}}}$
    
-   反向约束：$\sigma_w^2 \approx \frac{1}{n_{\text{out}}}$
    

**Xavier**（Glorot）取两者的**平均值**：

$$
\sigma_w^2 \approx \frac{2}{n_{\text{in}} + n_{\text{out}}}
$$

---

## **4️⃣ Xavier 初始化的公式**

对于均匀分布：

$$
w \sim U\left[-\sqrt{\frac{6}{n_{\text{in}}+n_{\text{out}}}},\; \sqrt{\frac{6}{n_{\text{in}}+n_{\text{out}}}}\right]
$$

对于正态分布：

$$
w \sim \mathcal{N}\left(0, \frac{2}{n_{\text{in}}+n_{\text{out}}}\right)
$$

---

### 💡 直觉小结

-   **输入维度**控制前向信号不变形
    
-   **输出维度**控制反向梯度不爆炸
    
-   Xavier 折中取二者平均
    
-   适合 **Sigmoid / Tanh**（激活范围有限，需要平衡正负值）
    

---

⚠️ **问题来了**：  
如果激活函数换成 ReLU，它会把一半的输出砍掉（负数变 0），导致方差下降一半，这时 Xavier 还合适吗？  
（A）依然合适  
（B）需要改公式补偿  
（C）随便啦，反正 BN 会帮我调  
你觉得选哪个？

You: 

b

ChatGPT: 

已思考 5s

答对了—选 **B** ✅  
因为 **ReLU 会把约一半的信号砍掉**（负数变 0），方差会下降一半，需要在初始化时**把方差翻倍补回去**。这就得到 **He 初始化**。

---

# 1) 为什么 ReLU 需要“加倍”方差？

设前一层线性输出 $z=\sum_{i=1}^{n_{\text{in}}} w_i x_i$，且 $z$ 近似零均值对称。  
对 ReLU：$y=\max(0,z)$

-   有 $\mathbb{P}(z>0)\approx 1/2$
    
-   $\mathbb{E}[y^2] = \mathbb{E}[z^2\mathbf{1}_{z>0}] \approx \tfrac{1}{2}\mathbb{E}[z^2] = \tfrac{1}{2}\mathrm{Var}[z]$
    

为保持前向方差稳定，需 $\mathrm{Var}[y]\approx \mathrm{Var}[x]$。  
而 $\mathrm{Var}[y]\approx \tfrac{1}{2}\mathrm{Var}[z] = \tfrac{1}{2}\, n_{\text{in}}\mathrm{Var}[w]\mathrm{Var}[x]$。  
令其 ≈ $\mathrm{Var}[x]$ ⇒ $\mathrm{Var}[w] \approx \dfrac{2}{n_{\text{in}}}$.

> **核心句子**：“ReLU 砍半方差 ⇒ 权重方差补到 $2/n_{\text{in}}$。”

---

# 2) He（Kaiming）初始化的公式

-   **正态版**：$w \sim \mathcal{N}\!\left(0,\; \dfrac{2}{n_{\text{in}}}\right)$
    
-   **均匀版**：$w \sim U\!\left[-\sqrt{\dfrac{6}{n_{\text{in}}}},\; \sqrt{\dfrac{6}{n_{\text{in}}}}\right]$
    

适用：\*\*ReLU / ReLU6 / PReLU / GELU（近似也常用）\*\*等以 0 为下截断或近似截断的激活。

---

# 3) 带泄漏的 ReLU（Leaky ReLU, slope = $a$）

若负半轴不过零而是 $ax$（例如 $a=0.01$），则激活不会砍掉那么多方差。  
推导可得近似：

$$
\mathrm{Var}[w] \approx \frac{2}{(1+a^2)\,n_{\text{in}}}
$$

$a=0$ 就退化回 ReLU 的 $2/n_{\text{in}}$。

> **核心句子**：“泄漏越大，砍掉的方差越少，权重方差系数从 2 降到 $2/(1+a^2)$。”

---

# 4) 和 Xavier/LeCun 的定位对比（速记表）

-   **Sigmoid/Tanh**：用 **Xavier**：$\mathrm{Var}[w]\approx \dfrac{2}{n_{\text{in}}+n_{\text{out}}}$（保持前后向折中）
    
-   **ReLU 家族**：用 **He**：$\mathrm{Var}[w]\approx \dfrac{2}{n_{\text{in}}}$（补回砍半方差）
    
-   **SELU**：用 **LeCun**：$\mathrm{Var}[w]\approx \dfrac{1}{n_{\text{in}}}$（与 SELU 的自归一化机制配套）
    

---

# 5) 工程落地（PyTorch 快记）

```python
import torch.nn as nn
import torch.nn.init as init

# ReLU 层
init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity='relu')  # 或 kaiming_uniform_
init.zeros_(layer.bias)

# Tanh/Sigmoid 层
init.xavier_uniform_(layer.weight)  # 或 xavier_normal_
```

---

### 小复习（1 句话）

-   **ReLU → He**；**Tanh/Sigmoid → Xavier**；**SELU → LeCun**。
    

---

**轮到你**（只要选一个）：  
如果某全连接层的 `fan_in = 512`，激活函数是 **ReLU**，那合适的权重方差应接近：  
A) $1/512$ B) $2/512$ C) $2/(512+512)$  
你选哪个？

You: 

b

ChatGPT: 

对的，选 **B) $2/512$** 🎯

因为这是 **He 初始化** 的标准公式：

$$
\mathrm{Var}[w] \approx \frac{2}{n_{\text{in}}}
$$

在这里 $n_{\text{in}}=512$，所以就是 $2/512$。

---

要不要我接下来帮你做一张 **“权重初始化选择指南”对照表** 📊，  
把激活函数 ↔ 推荐初始化方式 ↔ 公式 全整理出来？这样面试时你秒查就行。

You: 

ok

ChatGPT: 

好，那我们来做一张**权重初始化选择指南📊**，保证你面试时一眼就能定位到合适的方法。

---

## **权重初始化对照表** 🗂️

| 激活函数类型 | 推荐初始化方法 | 方差公式 $Var[w]$ | 均匀分布范围 | 正态分布标准差 | 适用场景 |
| --- | --- | --- | --- | --- | --- |
| **Sigmoid / Tanh** | **Xavier (Glorot)** | $\frac{2}{n_{\text{in}} + n_{\text{out}}}$ | $\pm \sqrt{\frac{6}{n_{\text{in}}+n_{\text{out}}}}$ | $\sqrt{\frac{2}{n_{\text{in}}+n_{\text{out}}}}$ | 饱和型激活，需前后向折中 |
| **ReLU / ReLU6** | **He (Kaiming)** | $\frac{2}{n_{\text{in}}}$ | $\pm \sqrt{\frac{6}{n_{\text{in}}}}$ | $\sqrt{\frac{2}{n_{\text{in}}}}$ | 截断负半轴，方差砍半需补偿 |
| **Leaky ReLU (slope=a)** | **He (Leaky)** | $\frac{2}{(1+a^2)\,n_{\text{in}}}$ | $\pm \sqrt{\frac{6}{(1+a^2)\,n_{\text{in}}}}$ | $\sqrt{\frac{2}{(1+a^2)\,n_{\text{in}}}}$ | 负半轴不过零，方差砍得少 |
| **SELU** | **LeCun** | $\frac{1}{n_{\text{in}}}$ | $\pm \sqrt{\frac{3}{n_{\text{in}}}}$ | $\sqrt{\frac{1}{n_{\text{in}}}}$ | 自归一化网络，配合 AlphaDropout |
| **线性 / 无激活** | **Xavier 或 LeCun** | 依需求选 | 同上 | 同上 | 线性变换或残差支路 |

---

### 🎯 面试速记口诀

> **“Sigmoid找Xavier，ReLU找He，SELU找LeCun”**  
> （加料版：Leaky ReLU → He / (1+a²)）

---

### 🔧 PyTorch 一行搞定

```python
import torch.nn.init as init

# Xavier
init.xavier_uniform_(layer.weight)

# He for ReLU
init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity='relu')

# LeCun
init.normal_(layer.weight, std=(1.0/layer.in_features)**0.5)
```

---

我建议我们接下来可以进入 **“高阶初始化技巧”** 部分，比如：

-   为什么 **Transformer** 的初始化会特殊调节（LayerNorm + 小初值）
    
-   大模型中如何避免 FP16 下溢 / 上溢
    
-   微调时是否重置权重
    

这样你就能覆盖**面试 + 实战**两方面。  
你想先看 **Transformer 初始化逻辑** 吗？这个在大厂面试很容易问到。

You: 

ok

ChatGPT: 

好，那我们来拆解一下 **Transformer 的初始化逻辑** 🧠💡，看看它为什么和普通 MLP 不一样。

---

## **1️⃣ Transformer 初始化的特殊背景**

Transformer 结构中有：

-   多头注意力（Multi-Head Attention, MHA）
    
-   前馈层（Feed-Forward Network, FFN）
    
-   残差连接（Residual Connection）
    
-   层归一化（LayerNorm）
    

这些组件对**初始化**有额外要求，因为：

1.  残差连接会把多层的输出直接相加，如果方差没控制好，值会越来越大。
    
2.  LayerNorm 会缩放和偏移输出，如果权重初值过大，会导致训练初期更新不稳定。
    
3.  MHA 里 $Q, K, V$ 的缩放（$1/\sqrt{d_k}$）会影响梯度方差。
    

---

## **2️⃣ 常见初始化策略**

以 **BERT** 为例：

-   **权重分布**：所有线性层的权重初始为 $\mathcal{N}(0, 0.02^2)$（很小的标准差）
    
-   **偏置**：初始化为 0（保证对称打破但不引入额外偏置）
    
-   **LayerNorm**：$\gamma$ 初始化为 1，$\beta$ 初始化为 0
    
-   **词嵌入矩阵**：与其他权重同样的 $\mathcal{N}(0, 0.02^2)$
    
-   **位置编码**：固定初始化（正弦或可学习的小值）
    

> **核心句子**：  
> “Transformer 初始化更关注控制方差累积，防止残差路径信号膨胀。”

---

## **3️⃣ 额外技巧**

-   **缩放残差输出**：在某些实现中，会在残差支路乘以 $\frac{1}{\sqrt{2}}$ 或更小系数，让多分支相加时不爆方差。
    
-   **参数预缩放**：比如 T5 在初始化时会让 FFN 输出的权重缩放 $1/d_{\text{model}}$。
    
-   **按层衰减学习率（Layer-wise LR decay）**：虽然不是初始化，但常和初始化策略一起使用，尤其在微调大模型时。
    

---

## **4️⃣ 与普通初始化的区别**

| 特性 | 普通 MLP 初始化 | Transformer 初始化 |
| --- | --- | --- |
| 方差控制 | Xavier / He 依激活 | 统一小方差（如 0.02）避免残差放大 |
| 归一化层 | 不一定有 | LayerNorm 贯穿始终，初始化为恒等变换 |
| 多分支相加 | 少见 | 常见（残差+注意力+FFN），需额外缩放 |
| 嵌入矩阵 | 可大可小 | 小方差+共享权重（embedding ↔ softmax） |

---

💡 **面试小提醒**  
如果被问到“为什么 BERT 初始化权重这么小”，你可以答：

> 因为 Transformer 堆叠层数多且有残差连接，如果用 Xavier/He 方差会累积放大，小方差能稳定训练初期的梯度和激活范围。

---

我觉得我们可以在下一步补一个 **“初始化 & BatchNorm / LayerNorm 配合原则”**，因为很多人训练崩了就是因为这块没处理好。  
你要继续看这个吗？这样我们的**权重初始化体系**就几乎全覆盖了。

You: 

ok

ChatGPT: 

好，那我们来聊聊 **初始化 & BatchNorm / LayerNorm 配合原则** 🛠️

---

## **1️⃣ BatchNorm 的影响**

BatchNorm（BN）在每个 batch 内做：

$$
\hat{x} = \frac{x - \mu_{\text{batch}}}{\sigma_{\text{batch}}}
$$

然后再做：

$$
y = \gamma \hat{x} + \beta
$$

-   **效果**：无论权重初始化的方差是多少，BN 都会重新归一化到均值 0、方差 1（再乘以 γ）。
    
-   **结果**：BN 在一定程度上**降低了初始化的敏感性**，所以 BN 层前的权重初始化可以不用非常严格。
    
-   **工程实践**：BN 前的权重常用 **He 初始化**（配 ReLU）或 Xavier 初始化（配 Tanh/Sigmoid），但即便偏离一些也不会马上崩。
    

> **小坑点**：如果 γ 初始化太大（比如 > 2），可能会在训练初期造成梯度爆炸。

---

## **2️⃣ LayerNorm 的影响**

LayerNorm（LN）对每个样本内的所有通道做归一化：

$$
\hat{x} = \frac{x - \mu_{\text{layer}}}{\sigma_{\text{layer}}}
$$

-   **效果**：LN 不依赖 batch 统计量，所以在 Transformer、RNN 里更稳定。
    
-   **结果**：初始化时**不需要严格控制前向方差恒定**，因为 LN 会自动缩放。
    
-   **工程实践**：LN 的 $\gamma$ 初始化为 1，$\beta$ 初始化为 0；其前面的线性层权重通常设为小方差（比如 BERT 的 0.02）。
    

---

## **3️⃣ 初始化和归一化的协同原则**

| 场景 | 推荐策略 |
| --- | --- |
| **无归一化层** | 严格选择 Xavier / He / LeCun，确保前后向方差稳定 |
| **有 BatchNorm** | 激活函数还是按规则选初始化，但可以不用太敏感；BN 参数 γ 建议初始化接近 1 |
| **有 LayerNorm** | 权重方差可以更小（防止残差膨胀），LN 参数初始化为 γ=1, β=0 |
| **超深网络 + 归一化** | 可以用小方差（0.01~0.02）初始化 + 归一化，防止累积方差爆炸 |

---

## **4️⃣ 面试小记忆法**

> “BN/LN 像‘安全气囊’，会在每一层帮你重新调节信号的均值和方差，但**安全气囊不是护身符**，初值过大还是可能炸。”

---

我觉得下一步我们可以进入**高阶初始化技巧**最后一块：  
**动态初始化（training-aware initialization）** & **混合精度训练下的初始化坑点**。  
这样我们就完成一个从**理论 → 工程 → 高阶**的全链路学习。

你要继续吗？这样我们就可以画一个完整的 **神经网络权重初始化知识地图** 给你收尾。

You: 

ok

ChatGPT: 

好，那我们来收最后一块 **高阶初始化技巧** 🍒，补上动态初始化和混合精度下的坑点，最后我给你画一张**知识地图**。

---

## **1️⃣ 动态初始化（Training-aware Initialization）**

**思路**：不是在训练开始前一次性初始化，而是**根据训练状态动态调整**权重的初值或缩放。

常见形式：

-   **LSUV（Layer-Sequential Unit Variance）**
    
    -   先用标准初始化 → 前向一次 → 观测每层输出方差 → 动态调整权重缩放
        
    -   直到每层输出方差接近 1 才开始正式训练
        
    -   优点：收敛更快，尤其在没有 BN/LN 的深层网络中
        
-   **MetaInit（Meta-learning 初始化）**
    
    -   用元学习找到在多个任务上都表现好的初始化值（多任务迁移）
        

> **核心句子**：  
> “动态初始化相当于给权重做一次‘预热调音’，保证一开训就跑在稳定区间。”

---

## **2️⃣ 混合精度训练（FP16/BF16）下的初始化坑点**

在 FP16/BF16 中：

-   **动态范围更小**（FP16 大约 1e-5 到 65504）
    
-   如果初始化的权重过小，乘上输入后容易\*\*下溢（underflow）\*\*变成 0
    
-   如果初始化的权重过大，容易\*\*上溢（overflow）\*\*成 inf 或 NaN
    

应对策略：

1.  **缩小方差但不过小**（如 BERT 0.02 → 0.01 需谨慎）
    
2.  配合 **loss scaling**（混合精度框架常自带）
    
3.  避免大范围均匀分布初始化（容易溢出）
    
4.  对 embedding 等大矩阵，初始化更要保守（范围 0.01~0.02）
    

---

## **3️⃣ 高阶经验小结**

-   训练有归一化层 → 初始方差可适当放宽
    
-   无归一化层且超深 → 考虑 LSUV
    
-   混合精度 → 注意动态范围，选保守方差 + loss scaling
    
-   大模型 → 方差控制比激活匹配更重要（残差会放大一切）
    

---

## **4️⃣ 神经网络权重初始化知识地图 🗺️**

```bash
目标：保持前后向方差稳定 & 打破对称性
│
├── 基础初始化
│   ├─ Xavier：Sigmoid/Tanh（Var=2/(n_in+n_out)）
│   ├─ He：ReLU（Var=2/n_in）
│   ├─ LeCun：SELU（Var=1/n_in）
│
├── 特殊结构
│   ├─ Transformer：小方差(0.02)，防残差爆炸
│   ├─ RNN/LSTM：Orthogonal 初始化（防梯度消失）
│
├── 配合归一化
│   ├─ BN：激活匹配初始化即可，γ≈1
│   ├─ LN：方差可更小，γ=1, β=0
│
├── 高阶技巧
│   ├─ 动态初始化：LSUV / MetaInit
│   ├─ 混合精度：控制范围 + loss scaling
│
└── 速记口诀
    "Sigmoid找Xavier，ReLU找He，SELU找LeCun"
```

---